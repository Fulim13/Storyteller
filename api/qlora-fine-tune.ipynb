{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto-gptq\n",
      "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: sentencepiece in ./myenv/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in ./myenv/lib/python3.10/site-packages (from auto-gptq) (1.1.1)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.10/site-packages (from auto-gptq) (4.66.5)\n",
      "Requirement already satisfied: transformers>=4.31.0 in ./myenv/lib/python3.10/site-packages (from auto-gptq) (4.46.3)\n",
      "Requirement already satisfied: peft>=0.5.0 in ./myenv/lib/python3.10/site-packages (from auto-gptq) (0.13.2)\n",
      "Requirement already satisfied: datasets in ./myenv/lib/python3.10/site-packages (from auto-gptq) (3.1.0)\n",
      "Collecting gekko\n",
      "  Downloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in ./myenv/lib/python3.10/site-packages (from auto-gptq) (2.5.1)\n",
      "Requirement already satisfied: safetensors in ./myenv/lib/python3.10/site-packages (from auto-gptq) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.26.2)\n",
      "Requirement already satisfied: psutil in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in ./myenv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./myenv/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./myenv/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.20.3)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in ./myenv/lib/python3.10/site-packages (from datasets->auto-gptq) (3.10.5)\n",
      "Requirement already satisfied: six in ./myenv/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (2.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
      "Installing collected packages: rouge, gekko, auto-gptq\n",
      "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.23.3-py3-none-any.whl (424 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.8.0 in ./myenv/lib/python3.10/site-packages (from optimum) (0.26.2)\n",
      "Requirement already satisfied: datasets in ./myenv/lib/python3.10/site-packages (from optimum) (3.1.0)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.10/site-packages (from optimum) (1.13.1)\n",
      "Requirement already satisfied: packaging in ./myenv/lib/python3.10/site-packages (from optimum) (24.1)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: transformers>=4.29 in ./myenv/lib/python3.10/site-packages (from optimum) (4.46.3)\n",
      "Requirement already satisfied: torch>=1.11 in ./myenv/lib/python3.10/site-packages (from optimum) (2.5.1)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.10/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.16.1)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.5.8)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./myenv/lib/python3.10/site-packages (from transformers>=4.29->optimum) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./myenv/lib/python3.10/site-packages (from transformers>=4.29->optimum) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.10/site-packages (from transformers>=4.29->optimum) (2024.7.24)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: pandas in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (3.10.5)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./myenv/lib/python3.10/site-packages (from datasets->optimum) (18.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (2.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./myenv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: humanfriendly, coloredlogs, optimum\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.23.3\n",
      "Requirement already satisfied: bitsandbytes in ./myenv/lib/python3.10/site-packages (0.44.1)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: torch in ./myenv/lib/python3.10/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: fsspec in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./myenv/lib/python3.10/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.10/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq\n",
    "!pip install optimum\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",  # automatically figures out how to best use CPU + GPU for loading model\n",
    "                                             trust_remote_code=False,  # prevents running custom model files on your machine\n",
    "                                             revision=\"main\")  # which version of model to use in repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Create a story with genre, title, characters and story [/INST] Title: The Enchanted Quill: A Tale of Magic and Ink\n",
      "\n",
      "Genre: Fantasy\n",
      "\n",
      "Story:\n",
      "\n",
      "In the mystical land of Inkasia, nestled between the mountains and the sea, there was a legendary artifact known as the Enchanted Quill. This quill was said to possess the power to bring any words written with it to life. The people of Inkasia lived in awe of this magical tool, and many a tale had been spun about its origins and the adventures of those who wielded it.\n",
      "\n",
      "Our story begins with the humble scribe, Elian, who lived in\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # model in evaluation mode (dropout modules are deactivated)\n",
    "\n",
    "# set padding token to end of string token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "story = \"Create a story with genre, title, characters and story\"\n",
    "prompt = f'''[INST] {story} [/INST]'''\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# generate output\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m140\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/projects/Storyteller/api/myenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/Storyteller/api/myenv/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/Storyteller/api/myenv/lib/python3.10/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/Storyteller/api/myenv/lib/python3.10/site-packages/transformers/generation/utils.py:2413\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2413\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"[INST] Create a story based on the given genre and title.\n",
    "input:\n",
    "    \"Genre: Romance\"\n",
    "    \"Title: Love Story Between James and Lucy\" [/INST]\n",
    "\"output:\n",
    "    \"Characters: <Generate the characters based on title (if provided, if not based on your creativity)\n",
    "    \"Story\" <Generate stories based on genre, title and characters>\n",
    "Stop Here, Only 1 story is enough, generate as long as you need, and only return the story\n",
    "\"\"\"\n",
    "\n",
    "# set padding token to end of string token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate output\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # model in training mode (dropout modules are activated)\n",
    "\n",
    "# enable gradient check pointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# enable quantized training\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA trainable version of model\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# trainable parameter count\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset with specified split\n",
    "dataset = load_dataset('FareedKhan/1k_stories_100_genre', split=\"train\")\n",
    "\n",
    "# Shuffle and select a range\n",
    "dataset = dataset.shuffle(seed=42).select(range(10))\n",
    "\n",
    "# Load NER pipeline\n",
    "model_name = \"dslim/bert-base-NER\"  # Pre-trained NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "# Function to extract characters using NER\n",
    "\n",
    "\n",
    "def extract_characters(example):\n",
    "    story = example['story']\n",
    "    entities = ner_pipeline(story)  # Run NER on the story\n",
    "    # Filter out only PERSON entities and ensure uniqueness\n",
    "    characters = list(\n",
    "        set(entity['word'] for entity in entities if entity['entity_group'] == 'PER'))\n",
    "    return {\"characters\": characters}\n",
    "\n",
    "\n",
    "# Apply the function to add the characters column\n",
    "dataset = dataset.map(extract_characters)\n",
    "\n",
    "# Function to format the prompt\n",
    "\n",
    "\n",
    "def format_prompt_alpaca(example):\n",
    "    \"\"\"\n",
    "    Create a structured and guiding prompt for story generation in Alpaca format,\n",
    "    ensuring characters and story are both part of the output.\n",
    "    \"\"\"\n",
    "    # Extract data from the example\n",
    "    genre = example[\"genre\"]\n",
    "    title = example[\"title\"]\n",
    "    characters = \", \".join(example[\"characters\"])\n",
    "    story = example[\"story\"]\n",
    "\n",
    "    # Create the Alpaca-style prompt with instruction and input-output fields\n",
    "    prompt = (\n",
    "        \"instruction: Generate a story based on the provided genre, title. Ensure the output includes the characters and the story.\"\n",
    "        \"input: (\"\n",
    "        f\"Genre: {genre}\\n\"\n",
    "        f\"Title: {title}\\n\"\n",
    "        \"),\"\n",
    "        \"output: (\"\n",
    "        f\"Characters: {characters}\\n\"\n",
    "        f\"Story: {story}\"\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Apply formatting and retain only {\"text\": prompt}\n",
    "dataset = dataset.map(format_prompt_alpaca, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the final dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# data collator\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "lr = 2e-4\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "\n",
    "# define training arguments\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"stories-result\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# train model\n",
    "# silence the warnings. Please re-enable for inference!\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "# renable warnings\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST] Create a story based on the given genre and title.\n",
    "input:\n",
    "    \"Genre: Romance\"\n",
    "    \"Title: Love Story Between James and Lucy\" [/INST]\n",
    "\"output:\n",
    "    \"Characters: <Generate the characters based on title (if provided, if not based on your creativity)\n",
    "    \"Story\" <Generate stories based on genre, title and characters>\n",
    "Stop Here, Only 1 story is enough, generate as long as you need, and only return the story\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
