{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate peft bitsandbytes transformers trl sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModelForSeq2SeqLM, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'story', 'genre', 'characters'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "# Replace with your dataset name\n",
    "dataset = load_dataset('FareedKhan/1k_stories_100_genre')\n",
    "\n",
    "# Load NER pipeline\n",
    "model_name = \"dslim/bert-base-NER\"  # Pre-trained NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", model=model, tokenizer=tokenizer, grouped_entities=True, device=0)\n",
    "\n",
    "# Function to extract characters using NER\n",
    "\n",
    "\n",
    "def extract_characters(example):\n",
    "    story = example['story']\n",
    "    entities = ner_pipeline(story)  # Run NER on the story\n",
    "   # Filter out only PERSON entities and ensure uniqueness\n",
    "    characters = list(\n",
    "        set(entity['word'] for entity in entities if entity['entity_group'] == 'PER'))\n",
    "    return {\"characters\": characters}\n",
    "\n",
    "\n",
    "# Apply the function to add the characters column\n",
    "dataset = dataset.map(extract_characters)\n",
    "\n",
    "# Inspect the updated dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Evelyn',\n",
       "  'Eve',\n",
       "  'Art',\n",
       "  'Reynolds',\n",
       "  'Hart',\n",
       "  'John',\n",
       "  'Turner',\n",
       "  'Amelia Hart',\n",
       "  'Arthur',\n",
       "  'Simmons',\n",
       "  'Jack'],\n",
       " ['Thorn', 'O'],\n",
       " ['Johnath', 'John'],\n",
       " ['Blackwood', 'Thomas', 'William', 'Elias'],\n",
       " ['Ben', 'Katie', 'Sarah', 'Mark', 'Alex']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['characters'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(example):\n",
    "    \"\"\"Create a structured and guiding prompt for story generation.\"\"\"\n",
    "\n",
    "    # Extract data from the example\n",
    "    genre = example[\"genre\"]\n",
    "    # Join characters into a single string\n",
    "    characters = \", \".join(example[\"characters\"])\n",
    "    story = example[\"story\"]\n",
    "    title = example[\"title\"]\n",
    "\n",
    "    # Create the prompt text with a guiding introduction\n",
    "    prompt = (\n",
    "        \"Create a story based on the given genre and title. \"\n",
    "        f\"Genre: {genre}\\n\"\n",
    "        f\"Title:\\n{title}\\n\\n\"\n",
    "        \"Ensure the output includes the following in order:\\n\"\n",
    "        \"1. Characters\\n\"\n",
    "        \"2. The story\\n\"\n",
    "        \"Output:\\n\"\n",
    "        f\"Characters: {characters}\\n\"\n",
    "        f\"Story: {story}\\n\"\n",
    "    )\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "\n",
    "# Apply the formatting to the dataset\n",
    "formatted_dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Create a story based on the given genre and title. Genre: Science Fiction\\nTitle:\\nThe Chronicles of the Cosmic Rift\\n\\nEnsure the output includes the following in order:\\n1. Characters\\n2. The story\\nOutput:\\nCharacters: Evelyn, Eve, Art, Reynolds, Hart, John, Turner, Amelia Hart, Arthur, Simmons, Jack\\nStory: In the year 2250, Earth had made significant strides in space exploration and interstellar travel. The United Earth Government (UEG) had established colonies on Mars, Jupiter\\'s moon Europa, and Saturn\\'s moon Titan. The advancements in technology and science had led to the creation of the Cosmic Rift Exploration Agency (CREA), a government-funded organization tasked with exploring the unknown regions of space and discovering new worlds and resources.\\n\\n    Dr. Amelia Hart, a brilliant astrophysicist, was the lead scientist at CREA\\'s headquarters on Luna. She had devoted her entire life to understanding the mysteries of the universe and had become a pioneer in her field. She was determined to uncover the secrets of the cosmic rifts, a series of mysterious and seemingly unconnected energy anomalies that had started appearing throughout the galaxy.\\n\\n    Dr. Hart assembled a diverse team of experts for her next mission, including her trusted second-in-command, Captain John \"Jack\" Reynolds, a seasoned astronaut and veteran of numerous CREA expeditions; Dr. Evelyn \"Eve\" Turner, a talented botanist and biologist; and Dr. Arthur \"Art\" Simmons, a genius engineer and inventor. Together, they would embark on a journey to explore the cosmic rift located in the Orion Arm of the Milky Way.\\n\\n    Their spacecraft, the SS Excelsior, was equipped with the latest technology, including a state-of-the-art cloaking device that would allow them to remain undetected as they ventured deeper into uncharted territory. As the Excelsior left the safety of Luna\\'s orbit, the crew was filled with anticipation and excitement. Little did they know that their journey would lead them to the edge of the known universe and beyond.\\n\\n    As they approached the cosmic rift, the Excelsior was suddenly engulfed by a powerful energy wave that shook the ship to its core. The crew struggled to maintain control, but the rift\\'s energy began to interfere with their systems. Just as the Excelsior was on the verge of being torn apart, Dr. Simmons managed to activate the ship\\'s emergency cloaking device. The rift\\'s energy pulsed around the Excelsior, but the ship remained hidden, protected by its advanced technology.\\n\\n    With their ship safely concealed, Dr. Hart and her team decided to send an exploration pod through the rift. As they ventured deeper into the unknown, they discovered a vast, twisted nebula filled with unimaginable wonders and dangers. They encountered alien life forms, exotic planets, and ancient artifacts that hinted at a long-lost civilization that had once ruled the galaxy.\\n\\n    Their journey took a dark turn when they encountered the malevolent and powerful entity known as the Enigma. This cosmic being, a creature of pure energy and malevolence, had been responsible for the creation of the cosmic rifts in an attempt to conquer the galaxy and enslave its inhabitants. The Enigma had been lurking in the shadows for millennia, feeding on the suffering and despair of its victims, and growing stronger with each passing moment.\\n\\n    As the crew of the Excelsior continued their exploration, they realized that they were not just uncovering the secrets of the cosmic rifts but also becoming entangled in the Enigma\\'s sinister plans. The Enigma had foreseen their arrival and had been manipulating their every move, using the crew\\'s own discoveries to its advantage. The more they learned about the cosmic rifts and the Enigma\\'s nefarious deeds, the more they became pawns in the Enigma\\'s twisted game.\\n\\n    Captain Reynolds, Dr. Hart, Dr. Turner, and Dr. Simmons would soon find themselves in a desperate struggle against the Enigma and its minions. They would face insurmountable odds and make sacrifices beyond imagination as they sought to save not only themselves but the entire galaxy from the Enigma\\'s tyrannical rule.\\n\\n    As the crew of the Excelsior fought valiantly against the Enigma\\'s forces, Dr. Hart made a stunning discovery. She uncovered a hidden prophecy that spoke of a hero who would rise to defeat the Enigma and restore balance to the universe. This hero, the prophecy revealed, would be born from the union of two powerful beings, one from Earth and one from the stars.\\n\\n    With this newfound knowledge, Dr. Hart and her team set out to locate the two beings who would become the parents of the prophesized hero. As they traversed the cosmic rift, they discovered that the Enigma had anticipated their actions and had already begun to hunt down the potential parents.\\n\\n    In a race against time, the crew of the Excelsior fought their way through the Enigma\\'s minions and protected the would-be parents at all costs. As they journeyed deeper into the rift, they found themselves confronted by the full might of the Enigma\\'s forces, including its deadly guardian, the Shadowbeast.\\n\\n    The Shadowbeast, a monstrous creature of pure energy and darkness, was a formidable foe that could tear apart planets and consume entire star systems. As the Excelsior and its crew faced off against the terrifying beast, they realized that they would need to combine their talents and resources in order to stand a chance against such an overwhelming adversary.\\n\\n    Dr. Hart, Dr. Turner, and Dr. Simmons devised a plan to outsmart the Enigma and its minions. They would use the SS Excelsior\\'s advanced technology to create a powerful weapon that could not only destroy the Shadowbeast but also sever the Enigma\\'s connection to the cosmic rift.\\n\\n    Captain Reynolds, the indomitable spirit of Earth\\'s pioneering spirit, would lead the crew in a daring assault on the Enigma\\'s stronghold, deep within the heart of the cosmic rift. They would infiltrate the fortress and sabotage its defenses, allowing them to strike a decisive blow against the Enigma and its sinister plans.\\n\\n    As they prepared for their final stand, the crew of the Excelsior knew that they were entering the most dangerous and uncertain period of their journey. They would face unimaginable horrors and make choices that would change the course of history. They would become heroes in the face of adversity, fighting for the future of the galaxy and the lives of countless innocents.\\n\\n    The battle that ensued was one of the most epic and brutal confrontations in the annals of galactic history. The crew of the Excelsior fought valiantly against the Enigma\\'s forces, using their wits, courage, and the power of their advanced technology to turn the tide of the battle.\\n\\n    Captain Reynolds, leading the charge, personally confronted the Shadowbeast in a fierce duel that would determine the fate of the galaxy. As the beast\\'s energy blasts tore through the Excelsior\\'s hull and threatened to consume the ship, Dr. Simmons managed to complete the construction of the weapon and activate its destructive power.\\n\\n    With a final, desperate lunge, Captain Reynolds plunged his ship into the heart of the Shadowbeast, detonating the weapon and unleashing a cataclysmic explosion that tore the beast apart and severed the Enigma\\'s connection to the cosmic rift.\\n\\n    As the dust settled and the Enigma\\'s forces crumbled under the weight of their own despair, the crew of the Excelsior emerged victorious. They had managed to save the galaxy from the Enigma\\'s tyrannical rule, and in doing so, had fulfilled the prophecy that had guided their journey.\\n\\n    The heroes of the Excelsior returned to Earth, their names etched in the annals of history as the saviors of the galaxy. Their incredible adventure had not only uncovered the secrets of the cosmic rifts but had also revealed the true power of the human spirit, the indomitable will to survive and the unbreakable bonds that forged the greatest heroes the galaxy had ever known.\\n\\n    And so, the SS Excelsior and its crew, led by the indomitable Captain Reynolds, sailed onward into the stars, their journey far from over. For they knew that there were still countless mysteries to uncover and challenges to face in the vast expanse of the cosmos. And as long as there were heroes to answer the call, the universe would remain a place of hope, wonder, and boundless possibilities.\\n', \"Create a story based on the given genre and title. Genre: Fantasy\\nTitle:\\nEldoria's Enchanted Whispers\\n\\nEnsure the output includes the following in order:\\n1. Characters\\n2. The story\\nOutput:\\nCharacters: Thorn, O\\nStory: In a land far away, where the sun shone brighter and the grass was greener, there existed a magical forest known as Eldoria. This enchanted forest was home to creatures of all shapes and sizes, each with their own unique abilities and personalities. The forest was a place of beauty and wonder, with its vibrant colors and mystical aura.\\n\\n    The story begins with a young boy named Thorn, who had just turned twelve. Thorn lived with his grandfather in a small cottage near the edge of Eldoria. His parents had vanished under mysterious circumstances when he was a baby, and his grandfather was all he had left in the world.\\n\\n    Thorn's grandfather was a wise and knowledgeable man, a former adventurer who had once explored the depths of Eldoria. He had passed down many tales of the forest to Thorn, filling his head with stories of brave heroes and fearsome beasts. Thorn's favorite of these tales was about the legendary Sword of Eldoria, a weapon of unimaginable power that could grant its wielder the ability to control the very elements of nature.\\n\\n    One day, while exploring the forest with his grandfather, Thorn stumbled upon a hidden cave. Inside the cave, he discovered an ancient scroll that spoke of the Sword of Eldoria and its whereabouts. The scroll revealed that the sword was hidden deep within the forest, guarded by a fearsome creature called the Shadow Beast.\\n\\n    Thorn's heart swelled with excitement and determination as he read the scroll. He knew that he must find the Sword of Eldoria and use its power to protect Eldoria from the evil forces that threatened to destroy it. His grandfather, seeing the determination in his eyes, agreed to help him on his quest.\\n\\n    And so, the journey began. Thorn and his grandfather traversed through the treacherous terrain of Eldoria, facing many challenges along the way. They encountered a variety of creatures, some friendly and some not so friendly.\\n\\n    One day, while crossing a rickety bridge over a deep ravine, they encountered a group of goblins. The goblins, who had been tormenting the forest's inhabitants, demanded that Thorn and his grandfather hand over their belongings. Thorn, armed with only his wits and the knowledge passed down by his grandfather, managed to outsmart the goblins and send them fleeing in terror.\\n\\n    As they continued their journey, Thorn and his grandfather found an ally in a wise old owl named Ollivia. Ollivia had lived in Eldoria for centuries and possessed a wealth of knowledge about the forest and its many secrets. She agreed to help them on their quest, providing them with valuable information and guidance.\\n\\n    With Ollivia's help, Thorn and his grandfather discovered the entrance to the Shadow Beast's lair, hidden deep within a dense thicket. As they approached the lair, they were met with a series of riddles and puzzles that they had to solve in order to proceed.\\n\\n    Thorn's intellect and problem-solving skills were put to the test as he navigated through the lair, solving each riddle and puzzle that stood in their way. Along the way, he also had to face a variety of traps and obstacles, which he managed to overcome with his quick thinking and agility.\\n\\n    Finally, after overcoming all the obstacles and solving the final riddle, Thorn and his companions found themselves face-to-face with the Shadow Beast. The beast, a monstrous creature shrouded in darkness, towered over them and unleashed a torrent of shadowy tendrils that threatened to engulf them.\\n\\n    Thorn, realizing that his wits alone would not be enough to defeat the beast, summoned the courage within him and reached for the Sword of Eldoria, which lay on a pedestal at the heart of the lair. As soon as his fingers touched the hilt of the sword, a brilliant light erupted from the blade, driving the Shadow Beast back and filling Thorn with newfound power.\\n\\n    With the Sword of Eldoria in his grasp, Thorn faced the Shadow Beast in a fierce battle of wills. He fought with the skill and precision of a master swordsman, channeling the power of the elements through the sword to unleash a torrent of lightning, fire, and wind upon the beast.\\n\\n    The Shadow Beast, though formidable, was no match for Thorn's newfound power. As the beast's dark energy began to falter, Thorn struck the final blow, driving the sword through its heart and banishing it to the shadows.\\n\\n    With the Shadow Beast defeated, Thorn and his companions returned to Eldoria, where they were hailed as heroes. The Sword of Eldoria was no longer hidden, and its power could now be used to protect the forest from any evil that dared to threaten it.\\n\\n    Thorn, now a fully-fledged hero, continued to live in the cottage near the edge of Eldoria with his grandfather. Together, they embarked on many more adventures, using the Sword of Eldoria to maintain the balance of nature and keep the forest safe from harm.\\n\\n    And so, the legend of Thorn, the hero of Eldoria, spread throughout the land, inspiring countless others to follow in his footsteps and protect the world from the forces of darkness.\\n\\n    The End\\n\\n    Good Twist: In the end, it is revealed that Thorn's parents were actually adventurers who had been sent on a mission to protect Eldoria from an ancient prophecy of darkness that foretold the rise of the Shadow Beast. Thorn's journey had not only fulfilled the prophecy but also brought his parents back into his life, as they had been trapped in the Shadow Beast's lair all these years.\\n\\n    Bad Twist: As time went on, Thorn began to feel the burden of his heroic deeds and the power of the Sword of Eldoria. One day, while out on a quest, he was confronted by a malevolent sorcerer who sought to take control of the sword for his own nefarious purposes. In a moment of weakness, Thorn willingly handed over the sword, believing that it was no longer worth the sacrifices he had to make to protect Eldoria.\\n\\n    Suspenseful Twist: Years after the defeat of the Shadow Beast, a new evil emerged in the form of a powerful sorceress who sought to bring chaos to Eldoria. Thorn, now an old man, was forced to once again take up the Sword of Eldoria and face his greatest challenge yet. Alongside his grandson, who had inherited his wisdom and courage, Thorn defeated the sorceress and restored peace to Eldoria, proving that the spirit of the hero would never fade.\\n\", \"Create a story based on the given genre and title. Genre: Mystery\\nTitle:\\nEchoes of Whispered Shadows\\n\\nEnsure the output includes the following in order:\\n1. Characters\\n2. The story\\nOutput:\\nCharacters: Johnath, John\\nStory: Once upon a time, in a small, tranquil town called Whispering Shadows, life seemed to move at a peaceful pace. The town was nestled in a picturesque valley, surrounded by towering mountains on all sides. The townsfolk were a close-knit community, living harmoniously, bound together by their shared love for their home and each other.\\n\\n    Our hero, Johnathan, was a young detective who had recently moved to Whispering Shadows, hoping to escape the chaos of the city and find some solace in the calm, idyllic town. He was a tall, lanky man in his late twenties, with dark hair and piercing blue eyes. Johnathan was well-respected among the townspeople, who admired his unwavering dedication to justice and his uncanny ability to solve even the most confounding mysteries.\\n\\n    As Johnathan settled into his new life, he quickly became aware of a series of strange occurrences that had been plaguing the town for years. The townsfolk whispered about these events in hushed tones, their voices barely above a whisper, as if the very shadows around them held a sinister secret.\\n\\n    One day, while strolling through the town square, Johnathan overheard two elderly ladies discussing a mysterious figure they had seen late at night. The figure, they said, seemed to vanish into thin air. Intrigued by the tale, Johnathan decided to investigate this enigmatic character.\\n\\n    As night fell, Johnathan ventured into the darkest corners of the town, keeping a sharp eye out for any signs of the elusive figure. Hours passed, and the night seemed to grow more ominous with each passing minute. The wind howled through the narrow streets, and the shadows seemed to take on a life of their own, creeping along the walls and floor like a thousand invisible fingers.\\n\\n    Just as Johnathan was about to give up on his quest, he spotted a figure standing at the edge of the town square. The figure was dressed all in black, with a hood that concealed their face. As Johnathan approached, the figure began to walk away, their footsteps barely audible as they disappeared into the night.\\n\\n    Determined to uncover the truth about this mysterious figure, Johnathan decided to follow them. As he trailed the figure through the darkened streets, he noticed that they seemed to have an uncanny ability to stay just out of reach, always remaining one step ahead.\\n\\n    After hours of relentless pursuit, Johnathan finally cornered the figure in an abandoned warehouse on the outskirts of town. As he entered the warehouse, he found himself in an eerie, dimly lit chamber, with only a single beam of light cutting through the darkness.\\n\\n    The figure turned to face Johnathan, their hood falling away to reveal their face. To Johnathan's shock, he found himself looking into the eyes of his long-lost brother, Thomas.\\n\\n    Thomas, it turned out, had been living in Whispering Shadows for years, struggling with a dark secret. As a child, he had been afflicted with a rare and incurable disease that had slowly robbed him of his ability to feel pain. In order to survive, Thomas had taken to stealing medicine from the local pharmacy, using the cover of darkness to slip in and out of the building unnoticed.\\n\\n    Overwhelmed with guilt, Thomas had withdrawn from the world, becoming a shadowy figure that haunted the town's nighttime landscape. He had hoped that by living a life of secrecy and solitude, he could protect his loved ones from the burden of his condition.\\n\\n    Moved by Thomas's plight, Johnathan vowed to help his brother find a cure for his disease. Together, they embarked on a quest that would take them to the farthest reaches of the earth, chasing after every lead and shred of hope.\\n\\n    As the years passed, the brothers' unwavering determination began to bear fruit. They discovered a cutting-edge research facility that was conducting groundbreaking experiments in pain management and neuroscience. Through their tireless efforts, they managed to secure a spot in the facility's clinical trial, giving Thomas a chance to regain his sense of feeling and reclaim his place in the world.\\n\\n    The trial was a success, and Thomas was finally able to experience the world without the veil of pain that had once held him captive. The brothers returned to Whispering Shadows, their hearts filled with gratitude and a newfound appreciation for the beauty of life.\\n\\n    The townsfolk, who had long suspected that the mysterious figure was a troubled soul in need of help, welcomed Thomas back with open arms. The brothers' triumphant return marked the end of the dark shadows that had once plagued their town, and the people of Whispering Shadows lived happily ever after.\\n\\n    The end.\\n\", \"Create a story based on the given genre and title. Genre: Historical Adventure\\nTitle:\\nEmerald Amulet Chronicles Revealed\\n\\nEnsure the output includes the following in order:\\n1. Characters\\n2. The story\\nOutput:\\nCharacters: Blackwood, Thomas, William, Elias\\nStory: Once upon a time in the 16th century, a small village nestled in the heart of the English countryside, far from the maddening crowd. The villagers, led by the wise and benevolent Mayor Thomas, lived in harmony, and their days were filled with laughter and joy. However, their peaceful existence was about to be shattered by a series of mysterious events that would test their faith and unity.\\n\\nThe village was known for its rich and fertile soil, which produced the finest crops. However, the villagers were not the only ones who recognized its value. The nearby town was ruled by the cunning and ruthless Lord Blackwood, who coveted the village's land for its wealth and potential.\\n\\nLord Blackwood, who was known for his cruel and tyrannical rule, had a grand vision to expand his kingdom and amass a fortune that would make him the most powerful man in the land. He saw the village as a stepping stone to achieving his goal, and he was not one to be deterred easily.\\n\\nTo achieve his ambition, Lord Blackwood dispatched his henchmen to infiltrate the village and undermine the villagers' unity. They began spreading lies and rumors, causing discord and mistrust among the villagers.\\n\\nMeanwhile, the village was blessed with a young hero, Thomas's son, William, a brave and intelligent young man who was admired by all. William had a deep sense of justice and a strong desire to protect his village and its people. He quickly became aware of the sinister plans of Lord Blackwood and his henchmen and vowed to do everything in his power to save his village.\\n\\nAs the conflict escalated, a mysterious traveler named Elias arrived in the village. Elias was a wise and enigmatic man, who claimed to possess ancient knowledge and powerful artifacts. He was on a quest to find the legendary Emerald Amulet, a powerful relic that was said to grant the wearer unimaginable power.\\n\\nElias sensed the impending danger that the village was facing and offered his assistance to William and the villagers. Together, they formed an alliance, and the race to find the Emerald Amulet began.\\n\\nThe trio embarked on a perilous journey through the dark and treacherous forests, facing countless challenges and overcoming unimaginable obstacles. As they ventured further, they discovered that the Emerald Amulet was not just an ordinary artifact but a symbol of hope and unity, and its powers were unleashed only when the villagers worked together.\\n\\nWith each step, the trio uncovered the dark secrets of the land and the hidden motives of the people they encountered. They battled the henchmen of Lord Blackwood and defeated them in epic showdowns, exposing their treachery and bringing them to justice.\\n\\nAs they got closer to the Emerald Amulet, they realized that the amulet was hidden within a sacred temple, guarded by a formidable dragon that had been bound by ancient magic. The dragon, sensing the pure intentions of the trio, agreed to release them from its bond if they could prove their worth and protect the Emerald Amulet from falling into the wrong hands.\\n\\nThe trio, now united with the mighty dragon, continued their journey, facing increasingly dangerous challenges and overcoming them with their newfound powers. However, they soon realized that Lord Blackwood was hot on their trail and had sent his most skilled and ruthless assassins to stop them at any cost.\\n\\nIn a final showdown, the trio, along with the dragon, confronted Lord Blackwood and his henchmen in a battle that would decide the fate of the village and the Emerald Amulet. The battle was fierce, and the stakes were higher than ever.\\n\\nAs the villagers watched the battle unfold from afar, they realized that their unity and determination were the true sources of their strength. They joined together, united by their common cause, and marched towards the battlefield to support their heroes.\\n\\nTogether, William, Elias, and the dragon fought valiantly, and the villagers' unwavering support gave them the strength to defeat Lord Blackwood and his henchmen. The dragon, as a token of gratitude, released its magical energy, transforming the Emerald Amulet into a symbol of unity and hope.\\n\\nWith the amulet in their possession, the villagers returned to their village, triumphant and united. The Emerald Amulet became the symbol of their unity, and the villagers vowed to protect it at all costs.\\n\\nThe story of the Emerald Amulet and the brave heroes who defended their village was passed down through generations, and their legacy lived on. The village continued to flourish, and the tale of their heroic deeds served as a reminder of the power of unity and the importance of standing up for what is right, no matter the cost.\\n\\nThe Chronicles of the Emerald Amulet is a tale of adventure, courage, and the indomitable spirit of unity that transcends time and space. It serves as a beacon of hope and inspiration for future generations, reminding them that the power to change the world lies within each of us, and that unity is the key to unlocking that power.\\n\", 'Create a story based on the given genre and title. Genre: Thriller\\nTitle:\\nThe Shadows of St. Augustine\\n\\nEnsure the output includes the following in order:\\n1. Characters\\n2. The story\\nOutput:\\nCharacters: Ben, Katie, Sarah, Mark, Alex\\nStory: In the sun-drenched coastal city of St. Augustine, Florida, a group of five friends found themselves entangled in a web of intrigue and danger. A tale of friendship, betrayal, and the fight for justice, the events that unfolded would forever change the lives of these seemingly ordinary individuals.\\n\\n    The protagonist, Alex, was a 26-year-old marine biologist, known for his passion for the ocean and its creatures. He had just moved to St. Augustine to start a new life and begin his career at the prestigious St. Augustine Marine Laboratory. Alex was a man of integrity, dedicated to his work and his friends.\\n\\n    Alex\\'s friends included Sarah, a talented artist and graphic designer who worked at a local advertising agency; Mark, a charming and charismatic young lawyer; Katie, a feisty and fearless journalist for the local newspaper; and Ben, a skilled computer programmer and hacker who worked remotely for a major tech company.\\n\\n    One fateful day, as the group gathered for their weekly get-together at a local beachfront bar, they stumbled upon a piece of information that would change their lives forever. A mysterious envelope, left on one of the bar stools, contained a series of cryptic messages, a map, and a photo of a man they had never seen before.\\n\\n    Intrigued, the group decided to investigate the matter, as they felt a strong sense of responsibility to uncover the truth. As they delved deeper into the mystery, they discovered that the man in the photo was a high-ranking government official, who had been involved in a series of illicit activities, including human trafficking and drug smuggling.\\n\\n    As they began to unravel the threads of this sinister web, they found themselves being hunted by a shadowy organization, determined to silence them and protect their secrets. The group realized that they had become pawns in a deadly game, and that the stakes were higher than they could have ever imagined.\\n\\n    In the midst of the chaos, the friends found strength in their bond, and they vowed to bring the corrupt official and his organization to justice, no matter the cost. They embarked on a perilous journey, navigating the treacherous waters of deception, betrayal, and danger.\\n\\n    As they continued their investigation, they discovered that the official had a vast network of accomplices, including crooked law enforcement officers, politicians, and businessmen. It became clear that the corruption extended far beyond what they had initially imagined, and that the organization was determined to maintain its stranglehold on the city and its people.\\n\\n    In a race against time, the friends uncovered a hidden underground facility, where the organization was holding several kidnapped women, including the daughter of a prominent local businessman. The group infiltrated the facility, battling through a gauntlet of armed guards and deadly traps, all while being pursued by the organization\\'s ruthless enforcer, a man known only as \"The Reaper.\"\\n\\n    With their lives on the line, the friends managed to rescue the kidnapped women and secure evidence of the organization\\'s crimes. However, as they emerged from the facility, they were confronted by The Reaper and a squad of armed mercenaries.\\n\\n    In a desperate standoff, the friends fought valiantly against their enemies, using their combined skills and expertise to turn the tide of battle in their favor. In the end, they managed to defeat The Reaper and his men, but not before suffering grievous wounds and losing their friend Mark, who had been fatally wounded in the process.\\n\\n    With the evidence in their possession, the friends turned to the local authorities for help. However, they were shocked to discover that many of the officials they had trusted were, in fact, members of the corrupt organization. Desperate, they turned to the one person they believed they could trust - the mysterious man from the photo, who had been the catalyst for their entire journey.\\n\\n    The man, whose name was revealed to be Thomas, was a former member of the organization who had gone rogue after discovering the extent of their corruption. He had been the one to leave the cryptic message at the bar, hoping that someone would uncover the truth and bring the organization to its knees.\\n\\n    Together, Alex, Sarah, Katie, and Ben, along with Thomas, worked tirelessly to expose the organization\\'s crimes and bring its leaders to justice. They faced numerous challenges and obstacles, but their determination and unwavering loyalty to one another kept them going, even in the darkest of times.\\n\\n    As the trial of the corrupt officials began, the group found themselves once again in the crosshairs of the organization\\'s deadly machinations. They discovered that the organization had infiltrated the judicial system, and they were determined to use their influence to ensure that the officials would walk free.\\n\\n    In a final showdown, the friends faced off against the organization\\'s leader, a man known as \"The Puppeteer,\" who was orchestrating the entire conspiracy from the shadows. In a thrilling climax, Alex and Thomas confronted The Puppeteer in his hidden lair, a sprawling underground complex filled with twisted devices and deadly traps.\\n\\n    As they battled their way through the labyrinthine complex, the friends discovered that The Puppeteer\\'s true identity was that of a long-lost relative of Alex\\'s, someone he had believed to be dead for many years. The revelation shattered Alex\\'s world, as he was forced to confront the truth about his past and the darkness that had been hiding within his own family.\\n\\n    In the end, Alex and Thomas managed to defeat The Puppeteer, but not before Alex was forced to make the ultimate sacrifice, taking a fatal blow in order to save his friend and bring an end to the organization\\'s reign of terror.\\n\\n    With the organization dismantled and the corrupt officials brought to justice, the friends were hailed as heroes by the people of St. Augustine. However, the weight of their losses and the sacrifices they had made weighed heavily on their hearts, and they knew that their lives would never be the same.\\n\\n    As they stood on the shores of the beautiful St. Augustine coastline, watching the sun set over the ocean, they vowed to honor the memory of their fallen friend Mark and to carry on his legacy of courage, integrity, and justice.\\n\\n    And so, the shadows of St. Augustine began to fade, as a new dawn of hope and redemption rose from the ashes of darkness and despair. The friends had triumphed against all odds, but they knew that their journey was far from over, and that the fight for justice would continue, no matter what challenges lay ahead.\\n']\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset['train']['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 18200.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset.save_to_disk(\"./datasets/processed_stories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'story', 'genre', 'characters', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset from the saved directory\n",
    "# Replace with the path you used earlier\n",
    "dataset = load_from_disk(\"./datasets/processed_stories\")\n",
    "\n",
    "# Inspect the loaded dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'story', 'genre', 'characters', 'text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example, index: index %\n",
    "                         100 == 0, with_indices=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=32,  # LoRA Scaling\n",
    "#     lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "#     r=64,  # Rank\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\n",
    "#         # Self-attention projection layers\n",
    "#         'self_attn.q_proj',\n",
    "#         'self_attn.k_proj',\n",
    "#         'self_attn.v_proj',\n",
    "#         'self_attn.o_proj',\n",
    "\n",
    "#         # MLP layers\n",
    "#         'mlp.gate_proj',\n",
    "#         'mlp.up_proj',\n",
    "#         'mlp.down_proj'\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=# Layers to target\n",
    "    ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "base_model\n",
      "base_model.model\n",
      "base_model.model.model\n",
      "base_model.model.model.embed_tokens\n",
      "base_model.model.model.layers\n",
      "base_model.model.model.layers.0\n",
      "base_model.model.model.layers.0.self_attn\n",
      "base_model.model.model.layers.0.self_attn.q_proj\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.k_proj\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.v_proj\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.o_proj\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.rotary_emb\n",
      "base_model.model.model.layers.0.mlp\n",
      "base_model.model.model.layers.0.mlp.gate_proj\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.mlp.up_proj\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.mlp.down_proj\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.mlp.act_fn\n",
      "base_model.model.model.layers.0.input_layernorm\n",
      "base_model.model.model.layers.0.post_attention_layernorm\n",
      "base_model.model.model.layers.1\n",
      "base_model.model.model.layers.1.self_attn\n",
      "base_model.model.model.layers.1.self_attn.q_proj\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.k_proj\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.v_proj\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.o_proj\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.rotary_emb\n",
      "base_model.model.model.layers.1.mlp\n",
      "base_model.model.model.layers.1.mlp.gate_proj\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.mlp.up_proj\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.mlp.down_proj\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.mlp.act_fn\n",
      "base_model.model.model.layers.1.input_layernorm\n",
      "base_model.model.model.layers.1.post_attention_layernorm\n",
      "base_model.model.model.layers.2\n",
      "base_model.model.model.layers.2.self_attn\n",
      "base_model.model.model.layers.2.self_attn.q_proj\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.k_proj\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.v_proj\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.o_proj\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.rotary_emb\n",
      "base_model.model.model.layers.2.mlp\n",
      "base_model.model.model.layers.2.mlp.gate_proj\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.mlp.up_proj\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.mlp.down_proj\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.mlp.act_fn\n",
      "base_model.model.model.layers.2.input_layernorm\n",
      "base_model.model.model.layers.2.post_attention_layernorm\n",
      "base_model.model.model.layers.3\n",
      "base_model.model.model.layers.3.self_attn\n",
      "base_model.model.model.layers.3.self_attn.q_proj\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.k_proj\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.v_proj\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.o_proj\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.rotary_emb\n",
      "base_model.model.model.layers.3.mlp\n",
      "base_model.model.model.layers.3.mlp.gate_proj\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.mlp.up_proj\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.mlp.down_proj\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.mlp.act_fn\n",
      "base_model.model.model.layers.3.input_layernorm\n",
      "base_model.model.model.layers.3.post_attention_layernorm\n",
      "base_model.model.model.layers.4\n",
      "base_model.model.model.layers.4.self_attn\n",
      "base_model.model.model.layers.4.self_attn.q_proj\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.k_proj\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.v_proj\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.o_proj\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.rotary_emb\n",
      "base_model.model.model.layers.4.mlp\n",
      "base_model.model.model.layers.4.mlp.gate_proj\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.mlp.up_proj\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.mlp.down_proj\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.mlp.act_fn\n",
      "base_model.model.model.layers.4.input_layernorm\n",
      "base_model.model.model.layers.4.post_attention_layernorm\n",
      "base_model.model.model.layers.5\n",
      "base_model.model.model.layers.5.self_attn\n",
      "base_model.model.model.layers.5.self_attn.q_proj\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.k_proj\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.v_proj\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.o_proj\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.rotary_emb\n",
      "base_model.model.model.layers.5.mlp\n",
      "base_model.model.model.layers.5.mlp.gate_proj\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.mlp.up_proj\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.mlp.down_proj\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.mlp.act_fn\n",
      "base_model.model.model.layers.5.input_layernorm\n",
      "base_model.model.model.layers.5.post_attention_layernorm\n",
      "base_model.model.model.layers.6\n",
      "base_model.model.model.layers.6.self_attn\n",
      "base_model.model.model.layers.6.self_attn.q_proj\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.k_proj\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.v_proj\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.o_proj\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.rotary_emb\n",
      "base_model.model.model.layers.6.mlp\n",
      "base_model.model.model.layers.6.mlp.gate_proj\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.mlp.up_proj\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.mlp.down_proj\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.mlp.act_fn\n",
      "base_model.model.model.layers.6.input_layernorm\n",
      "base_model.model.model.layers.6.post_attention_layernorm\n",
      "base_model.model.model.layers.7\n",
      "base_model.model.model.layers.7.self_attn\n",
      "base_model.model.model.layers.7.self_attn.q_proj\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.k_proj\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.v_proj\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.o_proj\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.rotary_emb\n",
      "base_model.model.model.layers.7.mlp\n",
      "base_model.model.model.layers.7.mlp.gate_proj\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.mlp.up_proj\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.mlp.down_proj\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.mlp.act_fn\n",
      "base_model.model.model.layers.7.input_layernorm\n",
      "base_model.model.model.layers.7.post_attention_layernorm\n",
      "base_model.model.model.layers.8\n",
      "base_model.model.model.layers.8.self_attn\n",
      "base_model.model.model.layers.8.self_attn.q_proj\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.k_proj\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.v_proj\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.o_proj\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.rotary_emb\n",
      "base_model.model.model.layers.8.mlp\n",
      "base_model.model.model.layers.8.mlp.gate_proj\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.mlp.up_proj\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.mlp.down_proj\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.mlp.act_fn\n",
      "base_model.model.model.layers.8.input_layernorm\n",
      "base_model.model.model.layers.8.post_attention_layernorm\n",
      "base_model.model.model.layers.9\n",
      "base_model.model.model.layers.9.self_attn\n",
      "base_model.model.model.layers.9.self_attn.q_proj\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.k_proj\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.v_proj\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.o_proj\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.rotary_emb\n",
      "base_model.model.model.layers.9.mlp\n",
      "base_model.model.model.layers.9.mlp.gate_proj\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.mlp.up_proj\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.mlp.down_proj\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.mlp.act_fn\n",
      "base_model.model.model.layers.9.input_layernorm\n",
      "base_model.model.model.layers.9.post_attention_layernorm\n",
      "base_model.model.model.layers.10\n",
      "base_model.model.model.layers.10.self_attn\n",
      "base_model.model.model.layers.10.self_attn.q_proj\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.k_proj\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.v_proj\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.o_proj\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.rotary_emb\n",
      "base_model.model.model.layers.10.mlp\n",
      "base_model.model.model.layers.10.mlp.gate_proj\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.mlp.up_proj\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.mlp.down_proj\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.mlp.act_fn\n",
      "base_model.model.model.layers.10.input_layernorm\n",
      "base_model.model.model.layers.10.post_attention_layernorm\n",
      "base_model.model.model.layers.11\n",
      "base_model.model.model.layers.11.self_attn\n",
      "base_model.model.model.layers.11.self_attn.q_proj\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.k_proj\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.v_proj\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.o_proj\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.rotary_emb\n",
      "base_model.model.model.layers.11.mlp\n",
      "base_model.model.model.layers.11.mlp.gate_proj\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.mlp.up_proj\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.mlp.down_proj\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.mlp.act_fn\n",
      "base_model.model.model.layers.11.input_layernorm\n",
      "base_model.model.model.layers.11.post_attention_layernorm\n",
      "base_model.model.model.layers.12\n",
      "base_model.model.model.layers.12.self_attn\n",
      "base_model.model.model.layers.12.self_attn.q_proj\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.k_proj\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.v_proj\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.o_proj\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.rotary_emb\n",
      "base_model.model.model.layers.12.mlp\n",
      "base_model.model.model.layers.12.mlp.gate_proj\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.mlp.up_proj\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.mlp.down_proj\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.mlp.act_fn\n",
      "base_model.model.model.layers.12.input_layernorm\n",
      "base_model.model.model.layers.12.post_attention_layernorm\n",
      "base_model.model.model.layers.13\n",
      "base_model.model.model.layers.13.self_attn\n",
      "base_model.model.model.layers.13.self_attn.q_proj\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.k_proj\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.v_proj\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.o_proj\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.rotary_emb\n",
      "base_model.model.model.layers.13.mlp\n",
      "base_model.model.model.layers.13.mlp.gate_proj\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.mlp.up_proj\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.mlp.down_proj\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.mlp.act_fn\n",
      "base_model.model.model.layers.13.input_layernorm\n",
      "base_model.model.model.layers.13.post_attention_layernorm\n",
      "base_model.model.model.layers.14\n",
      "base_model.model.model.layers.14.self_attn\n",
      "base_model.model.model.layers.14.self_attn.q_proj\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.k_proj\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.v_proj\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.o_proj\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.rotary_emb\n",
      "base_model.model.model.layers.14.mlp\n",
      "base_model.model.model.layers.14.mlp.gate_proj\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.mlp.up_proj\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.mlp.down_proj\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.mlp.act_fn\n",
      "base_model.model.model.layers.14.input_layernorm\n",
      "base_model.model.model.layers.14.post_attention_layernorm\n",
      "base_model.model.model.layers.15\n",
      "base_model.model.model.layers.15.self_attn\n",
      "base_model.model.model.layers.15.self_attn.q_proj\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.k_proj\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.v_proj\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.o_proj\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.rotary_emb\n",
      "base_model.model.model.layers.15.mlp\n",
      "base_model.model.model.layers.15.mlp.gate_proj\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.mlp.up_proj\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.mlp.down_proj\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.mlp.act_fn\n",
      "base_model.model.model.layers.15.input_layernorm\n",
      "base_model.model.model.layers.15.post_attention_layernorm\n",
      "base_model.model.model.layers.16\n",
      "base_model.model.model.layers.16.self_attn\n",
      "base_model.model.model.layers.16.self_attn.q_proj\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.k_proj\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.v_proj\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.o_proj\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.rotary_emb\n",
      "base_model.model.model.layers.16.mlp\n",
      "base_model.model.model.layers.16.mlp.gate_proj\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.mlp.up_proj\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.mlp.down_proj\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.mlp.act_fn\n",
      "base_model.model.model.layers.16.input_layernorm\n",
      "base_model.model.model.layers.16.post_attention_layernorm\n",
      "base_model.model.model.layers.17\n",
      "base_model.model.model.layers.17.self_attn\n",
      "base_model.model.model.layers.17.self_attn.q_proj\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.k_proj\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.v_proj\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.o_proj\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.rotary_emb\n",
      "base_model.model.model.layers.17.mlp\n",
      "base_model.model.model.layers.17.mlp.gate_proj\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.mlp.up_proj\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.mlp.down_proj\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.mlp.act_fn\n",
      "base_model.model.model.layers.17.input_layernorm\n",
      "base_model.model.model.layers.17.post_attention_layernorm\n",
      "base_model.model.model.layers.18\n",
      "base_model.model.model.layers.18.self_attn\n",
      "base_model.model.model.layers.18.self_attn.q_proj\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.k_proj\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.v_proj\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.o_proj\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.rotary_emb\n",
      "base_model.model.model.layers.18.mlp\n",
      "base_model.model.model.layers.18.mlp.gate_proj\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.mlp.up_proj\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.mlp.down_proj\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.mlp.act_fn\n",
      "base_model.model.model.layers.18.input_layernorm\n",
      "base_model.model.model.layers.18.post_attention_layernorm\n",
      "base_model.model.model.layers.19\n",
      "base_model.model.model.layers.19.self_attn\n",
      "base_model.model.model.layers.19.self_attn.q_proj\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.k_proj\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.v_proj\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.o_proj\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.rotary_emb\n",
      "base_model.model.model.layers.19.mlp\n",
      "base_model.model.model.layers.19.mlp.gate_proj\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.mlp.up_proj\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.mlp.down_proj\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.mlp.act_fn\n",
      "base_model.model.model.layers.19.input_layernorm\n",
      "base_model.model.model.layers.19.post_attention_layernorm\n",
      "base_model.model.model.layers.20\n",
      "base_model.model.model.layers.20.self_attn\n",
      "base_model.model.model.layers.20.self_attn.q_proj\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.k_proj\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.v_proj\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.o_proj\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.rotary_emb\n",
      "base_model.model.model.layers.20.mlp\n",
      "base_model.model.model.layers.20.mlp.gate_proj\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.mlp.up_proj\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.mlp.down_proj\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.mlp.act_fn\n",
      "base_model.model.model.layers.20.input_layernorm\n",
      "base_model.model.model.layers.20.post_attention_layernorm\n",
      "base_model.model.model.layers.21\n",
      "base_model.model.model.layers.21.self_attn\n",
      "base_model.model.model.layers.21.self_attn.q_proj\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.k_proj\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.v_proj\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.o_proj\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.rotary_emb\n",
      "base_model.model.model.layers.21.mlp\n",
      "base_model.model.model.layers.21.mlp.gate_proj\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_dropout\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.mlp.up_proj\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_dropout\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.mlp.down_proj\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_dropout\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.mlp.act_fn\n",
      "base_model.model.model.layers.21.input_layernorm\n",
      "base_model.model.model.layers.21.post_attention_layernorm\n",
      "base_model.model.model.norm\n",
      "base_model.model.model.rotary_emb\n",
      "base_model.model.lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/fulim/projects/Storyteller/api/myenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 231.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save QLoRA weights\n",
    "trainer.model.save_pretrained(\"Mistral-7B-Instruct-v0.3-QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"Mistral-7B-Instruct-v0.3-QLoRA\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"Mistral-7B-Instruct-v0.3-Merged\")\n",
    "tokenizer.save_pretrained(\"Mistral-7B-Instruct-v0.3-Merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"Create a story based on the given genre and title.\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\",\n",
    "                model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Create a story based on the given genre and title.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Move to GPU\n",
    "outputs = merged_model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
